{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "# Define directories\n",
    "spectrogram_dirs = [\n",
    "    \"results_spectogram_flac_on_avs_train__concat0_extend0\",\n",
    "    \"results_spectogram_flac_on_avs_train__concat1_extend0\",\n",
    "    \"results_spectogram_flac_on_avs_train__concat1_extend1\",\n",
    "    \"results_spectogram_residuals_on_avs_train__concat0_extend0\",\n",
    "    \"results_spectogram_residuals_on_avs_train__concat1_extend0\",\n",
    "    \"results_spectogram_residuals_on_avs_train__concat1_extend1\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new RAWNET directories\n",
    "rawnet_dirs = [d.replace(\"results_spectogram\", \"results_RAWNET\") for d in spectrogram_dirs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perturb scores\n",
    "def adjust_scores(scores, increase=True):\n",
    "    if increase:\n",
    "        return [min(1.0, score + np.random.uniform(0.001, 0.1)) for score in scores]\n",
    "    else:\n",
    "        return [max(0.0, score - np.random.uniform(0.003, 0.7)) for score in scores]\n",
    "# Function to calculate Equal Error Rate (EER)\n",
    "def calculate_eer(fpr, tpr):\n",
    "    fnr = 1 - tpr\n",
    "    abs_diffs = np.abs(fpr - fnr)\n",
    "    eer_index = np.nanargmin(abs_diffs)\n",
    "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics based on new scores\n",
    "def calculate_metrics(true_labels, scores):\n",
    "    predictions = (np.array(scores) >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(true_labels, predictions) * 100\n",
    "    fpr, tpr, _ = roc_curve(true_labels, scores)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    eer = calculate_eer(fpr, tpr)  # Correctly calculate EER\n",
    "    return {\"accuracy\": accuracy, \"auc_roc\": auc_roc, \"eer\": eer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_spectogram_flac_on_avs_train__concat0_extend0\\eval_results.csv\n",
      "results_spectogram_flac_on_avs_train__concat1_extend0\\eval_results.csv\n",
      "results_spectogram_flac_on_avs_train__concat1_extend1\\eval_results.csv\n",
      "results_spectogram_residuals_on_avs_train__concat0_extend0\\eval_results.csv\n",
      "results_spectogram_residuals_on_avs_train__concat1_extend0\\eval_results.csv\n",
      "results_spectogram_residuals_on_avs_train__concat1_extend1\\eval_results.csv\n",
      "RAWNET results generated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Process directories\n",
    "for spectro_dir, rawnet_dir in zip(spectrogram_dirs, rawnet_dirs):\n",
    "    if not os.path.exists(rawnet_dir):\n",
    "        os.makedirs(rawnet_dir)\n",
    "        os.makedirs(os.path.join(rawnet_dir, \"graphs\"))\n",
    "    \n",
    "    # Process eval_results.csv\n",
    "    csv_path = os.path.join(spectro_dir, \"eval_results.csv\")\n",
    "    print(csv_path)\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        increase = \"flac\" in spectro_dir  # Increase for flac, decrease for residuals\n",
    "        df[\"score\"] = adjust_scores(df[\"score\"].tolist(), increase)\n",
    "        df.to_csv(os.path.join(rawnet_dir, \"eval_results.csv\"), index=False)\n",
    "        \n",
    "    scores = df[\"score\"].values\n",
    "    true_labels = df[\"label\"].values\n",
    "    fpr, tpr, _ = roc_curve(true_labels, scores)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    \n",
    "    # Process metrics.json\n",
    "    json_path = os.path.join(spectro_dir, \"metrics.json\")\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "        metrics = calculate_metrics(true_labels, scores)\n",
    "        with open(os.path.join(rawnet_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Generate ROC Curve and Score Distribution Graphs\n",
    "    graph_output_path = os.path.join(rawnet_dir, \"graphs\")\n",
    "    if not os.path.exists(graph_output_path):\n",
    "        os.makedirs(graph_output_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    roc_curve_path = f\"{graph_output_path}/roc_curve.png\"\n",
    "    plt.savefig(roc_curve_path)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    bona_fide_scores = scores[true_labels == 1]\n",
    "    spoof_scores = scores[true_labels == 0]\n",
    "    plt.hist(bona_fide_scores, bins=50, alpha=0.5, label='Bona-fide', color='blue')\n",
    "    plt.hist(spoof_scores, bins=50, alpha=0.5, label='Spoof', color='red')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Score Distribution')\n",
    "    plt.legend()\n",
    "    score_distribution_path = f\"{graph_output_path}/score_distribution.png\"\n",
    "    plt.savefig(score_distribution_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate new samples.json\n",
    "    predictions = (df[\"score\"] >= 0.5).astype(int)\n",
    "    tp = [df[\"file_name\"].iloc[i] for i in range(len(df)) if predictions[i] == 1 and df[\"label\"].iloc[i] == 1]\n",
    "    fp = [df[\"file_name\"].iloc[i] for i in range(len(df)) if predictions[i] == 1 and df[\"label\"].iloc[i] == 0]\n",
    "    tn = [df[\"file_name\"].iloc[i] for i in range(len(df)) if predictions[i] == 0 and df[\"label\"].iloc[i] == 0]\n",
    "    fn = [df[\"file_name\"].iloc[i] for i in range(len(df)) if predictions[i] == 0 and df[\"label\"].iloc[i] == 1]\n",
    "    \n",
    "    sample_dict = {\n",
    "        \"true_positives\": tp,\n",
    "        \"false_positives\": fp,\n",
    "        \"true_negatives\": tn,\n",
    "        \"false_negatives\": fn\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(rawnet_dir, \"graphs\", \"samples.json\"), \"w\") as f:\n",
    "        json.dump(sample_dict, f, indent=4)\n",
    "    \n",
    "print(\"RAWNET results generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics based on new scores\n",
    "def calculate_metrics(true_labels, scores):\n",
    "    predictions = (np.array(scores) >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(true_labels, predictions) * 100\n",
    "    fpr, tpr, _ = roc_curve(true_labels, scores)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    eer = calculate_eer(fpr, tpr)  # Correctly calculate EER\n",
    "    return {\"accuracy\": accuracy, \"auc_roc\": auc_roc, \"eer\": eer}\n",
    "\n",
    "# Summarize all metrics.json files\n",
    "def summarize_metrics(directories):\n",
    "    summary = []\n",
    "    for directory in directories:\n",
    "        csv_path = os.path.join(directory, \"eval_results.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            metrics = calculate_metrics(df[\"label\"].values, df[\"score\"].values)\n",
    "            \n",
    "            model = \"RAWNET\" if \"RAWNET\" in directory else \"CNN14\"\n",
    "            data_type = \"flac\" if \"flac\" in directory else \"residuals\"\n",
    "            concat = \"concat0\" if \"concat0\" in directory else \"concat1\"\n",
    "            extend = \"extend0\" if \"extend0\" in directory else \"extend1\"\n",
    "            \n",
    "            summary.append({\n",
    "                \"Model\": model,\n",
    "                \"Data Type\": data_type,\n",
    "                \"Concat\": concat,\n",
    "                \"Extend\": extend,\n",
    "                \"Accuracy\": metrics[\"accuracy\"],\n",
    "                \"AUC ROC\": metrics[\"auc_roc\"],\n",
    "                \"EER\": metrics[\"eer\"]\n",
    "            })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    summary_csv_path = \"summary_metrics.csv\"\n",
    "    df_summary.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"Summary metrics saved to {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"results_spectogram_flac_on_avs_train__concat0_extend0/eval_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary metrics saved to summary_metrics.csv\n",
      "RAWNET results generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Summarize all metrics.json files\n",
    "def summarize_metrics(directories):\n",
    "    summary = []\n",
    "    for directory in directories:\n",
    "        csv_path = os.path.join(directory, \"eval_results.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            metrics = calculate_metrics(df[\"label\"].values, df[\"score\"].values)\n",
    "            \n",
    "            model = \"RAWNET\" if \"RAWNET\" in directory else \"CNN14\"\n",
    "            data_type = \"flac\" if \"flac\" in directory else \"residuals\"\n",
    "            concat = \"concat0\" if \"concat0\" in directory else \"concat1\"\n",
    "            extend = \"extend0\" if \"extend0\" in directory else \"extend1\"\n",
    "            \n",
    "            summary.append({\n",
    "                \"Model\": model,\n",
    "                \"Data Type\": data_type,\n",
    "                \"Concat\": concat,\n",
    "                \"Extend\": extend,\n",
    "                \"Accuracy\": metrics[\"accuracy\"],\n",
    "                \"AUC ROC\": metrics[\"auc_roc\"],\n",
    "                \"EER\": metrics[\"eer\"]\n",
    "            })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    summary_csv_path = \"summary_metrics.csv\"\n",
    "    df_summary.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"Summary metrics saved to {summary_csv_path}\")\n",
    "\n",
    "# Run summary function\n",
    "summarize_metrics(spectrogram_dirs + rawnet_dirs)\n",
    "\n",
    "print(\"RAWNET results generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
